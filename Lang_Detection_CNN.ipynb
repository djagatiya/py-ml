{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "# core libs\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Embedding, Conv1D, MaxPool1D, Flatten, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library versions\n",
    "print(f'keras= {keras.__version__}')\n",
    "print(f'sklearn= {sklearn.__version__}')\n",
    "print(f'numpy= {np.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Configs\n",
    "data_dir = '/media/divyesh/WorkSpace/Blogs/lang_detection/raw_data'\n",
    "# Select to articles from file\n",
    "num_of_articles = 10000\n",
    "# Maximum sequence length\n",
    "sentense_len = 150\n",
    "# shingle configs\n",
    "shingles_range = (70, 100, 130)\n",
    "# how many shingles generate per line\n",
    "shingle_per_line = 10 \n",
    "# out of vocabulary token\n",
    "oov_str = 'oov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language code wise full name mapping\n",
    "lang_code_dict = {\n",
    "    'en':'english', 'de':'german', \n",
    "    'fr':'french', 'it':'italian', \n",
    "    'es':'spanish'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language code wise data file mapping\n",
    "data_info = {\n",
    "    'en' : data_dir + '/en.txt',\n",
    "    'de' : data_dir + '/de.txt',\n",
    "    'fr' : data_dir + '/fr.txt',\n",
    "    'it' : data_dir + '/it.txt',\n",
    "    'es' : data_dir + '/es.txt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en english /media/divyesh/WorkSpace/Blogs/lang_detection/raw_data/en.txt\n",
      "de german /media/divyesh/WorkSpace/Blogs/lang_detection/raw_data/de.txt\n",
      "fr french /media/divyesh/WorkSpace/Blogs/lang_detection/raw_data/fr.txt\n",
      "it italian /media/divyesh/WorkSpace/Blogs/lang_detection/raw_data/it.txt\n",
      "es spanish /media/divyesh/WorkSpace/Blogs/lang_detection/raw_data/es.txt\n"
     ]
    }
   ],
   "source": [
    "for lang_code, file_path in data_info.items():\n",
    "    print(lang_code, lang_code_dict[lang_code], file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 10000\n",
      "de 10000\n",
      "fr 10000\n",
      "it 10000\n",
      "es 10000\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "data_dict = {}\n",
    "for lang_code, file_path in data_info.items():\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = lines[:num_of_articles]\n",
    "        # convert to lower case\n",
    "        lines = [l.lower().strip() for l in lines]\n",
    "        data_dict[lang_code] = lines\n",
    "        print(lang_code, len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shingles(line, length, total):\n",
    "    \"\"\"\n",
    "    Generate shingles from line\n",
    "    \"\"\"\n",
    "    #todo: USE SET to remove REDUUUUUU\n",
    "    shingle_list = [] \n",
    "    max_index = len(line) - length\n",
    "    if max_index > 0:\n",
    "        for _ in range(total):\n",
    "            index = random.randint(0, max_index)\n",
    "            shingle_text = line[index:index+length]\n",
    "            shingle_list.append(shingle_text)\n",
    "    else:\n",
    "        shingle_list.append(line)\n",
    "    return shingle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shingles_lines(line, length, total):\n",
    "    \"\"\"\n",
    "    Generate shingles from list of lines\n",
    "    \"\"\"\n",
    "    shingle_list = []\n",
    "    for line in lines:\n",
    "        shingles = generate_shingles(line=line, length=length, total=total)\n",
    "        shingle_list.extend(shingles)\n",
    "    return shingle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 300000\n",
      "de 300000\n",
      "fr 300000\n",
      "it 300000\n",
      "es 300000\n"
     ]
    }
   ],
   "source": [
    "# generate shingles\n",
    "shingle_data_dict = {}\n",
    "for lang, lines in data_dict.items():\n",
    "    shingle_list = []\n",
    "    for s_range in shingles_range:\n",
    "        shingles = generate_shingles_lines(lines, s_range, shingle_per_line)\n",
    "        shingle_list.extend(shingles)\n",
    "    shingle_data_dict[lang] = shingle_list\n",
    "    print(lang, len(shingle_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500000 1500000\n"
     ]
    }
   ],
   "source": [
    "# create list of lines and labels\n",
    "data_lines, labels = [], []\n",
    "for lang, samples in shingle_data_dict.items():\n",
    "    data_lines.extend(samples)\n",
    "    total_samples = len(samples)\n",
    "    labels.extend([lang] * total_samples)\n",
    "print(len(data_lines), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'e', 'a', 'i', 'n', 'r', 't', 's', 'o', 'l', 'd', 'c', 'u', 'm', 'p', 'g', 'h', 'b', 'f', 'v', 'w', 'z', 'y', 'k', 'é', 'q', 'j', 'x', 'ó', 'í', 'ü', 'á', 'ä', 'è', 'ö', 'à', 'ñ', 'ú', 'ß', 'ò', 'ù', 'ç', 'ê', 'ô', 'â', 'î', 'ì', 'œ', 'û', 'ï', 'ō', '²', 'š', 'ë', 'č', 'ł', 'ã', 'ø', 'ā', 'ć', 'ū', 'ı', 'ž', 'ř', 'ş', 'å', 'ý', 'æ', 'α', 'ο', 'ν', 'ă', 'о', 'ń', '_', 'ğ', 'oov']\n"
     ]
    }
   ],
   "source": [
    "# create list of all characters from all data lines\n",
    "data_char_ls = []\n",
    "for line in data_lines:\n",
    "    char_ls = [c for c in line]\n",
    "    data_char_ls.append(char_ls)\n",
    "    \n",
    "# count all characters\n",
    "cunt = Counter(x for xs in data_char_ls for x in set(xs))\n",
    "\n",
    "# create vocabulary\n",
    "char_vocab = [c[0] for c in cunt.most_common(76)] + [oov_str]\n",
    "print(char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'e': 2, 'a': 3, 'i': 4, 'n': 5, 'r': 6, 't': 7, 's': 8, 'o': 9, 'l': 10, 'd': 11, 'c': 12, 'u': 13, 'm': 14, 'p': 15, 'g': 16, 'h': 17, 'b': 18, 'f': 19, 'v': 20, 'w': 21, 'z': 22, 'y': 23, 'k': 24, 'é': 25, 'q': 26, 'j': 27, 'x': 28, 'ó': 29, 'í': 30, 'ü': 31, 'á': 32, 'ä': 33, 'è': 34, 'ö': 35, 'à': 36, 'ñ': 37, 'ú': 38, 'ß': 39, 'ò': 40, 'ù': 41, 'ç': 42, 'ê': 43, 'ô': 44, 'â': 45, 'î': 46, 'ì': 47, 'œ': 48, 'û': 49, 'ï': 50, 'ō': 51, '²': 52, 'š': 53, 'ë': 54, 'č': 55, 'ł': 56, 'ã': 57, 'ø': 58, 'ā': 59, 'ć': 60, 'ū': 61, 'ı': 62, 'ž': 63, 'ř': 64, 'ş': 65, 'å': 66, 'ý': 67, 'æ': 68, 'α': 69, 'ο': 70, 'ν': 71, 'ă': 72, 'о': 73, 'ń': 74, '_': 75, 'ğ': 76, 'oov': 77}\n",
      "\n",
      "{1: ' ', 2: 'e', 3: 'a', 4: 'i', 5: 'n', 6: 'r', 7: 't', 8: 's', 9: 'o', 10: 'l', 11: 'd', 12: 'c', 13: 'u', 14: 'm', 15: 'p', 16: 'g', 17: 'h', 18: 'b', 19: 'f', 20: 'v', 21: 'w', 22: 'z', 23: 'y', 24: 'k', 25: 'é', 26: 'q', 27: 'j', 28: 'x', 29: 'ó', 30: 'í', 31: 'ü', 32: 'á', 33: 'ä', 34: 'è', 35: 'ö', 36: 'à', 37: 'ñ', 38: 'ú', 39: 'ß', 40: 'ò', 41: 'ù', 42: 'ç', 43: 'ê', 44: 'ô', 45: 'â', 46: 'î', 47: 'ì', 48: 'œ', 49: 'û', 50: 'ï', 51: 'ō', 52: '²', 53: 'š', 54: 'ë', 55: 'č', 56: 'ł', 57: 'ã', 58: 'ø', 59: 'ā', 60: 'ć', 61: 'ū', 62: 'ı', 63: 'ž', 64: 'ř', 65: 'ş', 66: 'å', 67: 'ý', 68: 'æ', 69: 'α', 70: 'ο', 71: 'ν', 72: 'ă', 73: 'о', 74: 'ń', 75: '_', 76: 'ğ', 77: 'oov'}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary for (char to index)\n",
    "# here we (index + 1) becoz, 0 index for padding\n",
    "ch2int = {c:i+1 for i, c in enumerate(char_vocab)}\n",
    "print(ch2int)\n",
    "print()\n",
    "# create dictionary for (index to char)\n",
    "int2ch = {i:c for c, i in ch2int.items()}\n",
    "print(int2ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(in_ls, key):\n",
    "    \"\"\"\n",
    "    encode list of character to index of characters using 'char2int' dictionary\n",
    "    \"\"\"\n",
    "    out_ls = []\n",
    "    for ch in in_ls:\n",
    "        index = key.get(ch)\n",
    "        if index is None:\n",
    "            index = key.get(oov_str)\n",
    "        out_ls.append(index)\n",
    "    return out_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500000\n"
     ]
    }
   ],
   "source": [
    "# data encoding\n",
    "encoded_ls = [encode(l, ch2int) for l in data_lines]\n",
    "print(len(encoded_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14870"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe count 'oov' in dataset\n",
    "counts = 0\n",
    "for enc in encoded_ls:\n",
    "    if ch2int[oov_str] in enc:\n",
    "        counts += 1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding and trucating of encoded sequence\n",
    "X = pad_sequences(encoded_ls, maxlen=sentense_len, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de' 'en' 'es' 'fr' 'it']\n"
     ]
    }
   ],
   "source": [
    "# target encoding from 'en' or 'de' language code to 0, 1 \n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of targets\n",
    "y = to_categorical(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500000, 150) (1500000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Test split (70:30) ratio from full data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050000, 150) (450000, 150) (1050000, 5) (450000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 150, 64)           4992      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 146, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 29, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 25, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 50,565\n",
      "Trainable params: 50,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyesh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Build the Neural network\n",
    "inp = Input(shape=(sentense_len, ))\n",
    "x = Embedding(input_dim=len(char_vocab) + 1, output_dim=64)(inp)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = MaxPool1D(5)(x)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = MaxPool1D(20)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(5, activation='softmax')(x)\n",
    "model = Model(inputs=inp, output=x)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/divyesh/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1050000 samples, validate on 450000 samples\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 750s 715us/step - loss: 0.1605 - acc: 0.9531 - val_loss: 0.1075 - val_acc: 0.9710\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 753s 717us/step - loss: 0.0967 - acc: 0.9734 - val_loss: 0.0899 - val_acc: 0.9753\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 719s 685us/step - loss: 0.0850 - acc: 0.9765 - val_loss: 0.0859 - val_acc: 0.9765\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 720s 686us/step - loss: 0.0787 - acc: 0.9781 - val_loss: 0.0832 - val_acc: 0.9772\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 751s 715us/step - loss: 0.0744 - acc: 0.9792 - val_loss: 0.0808 - val_acc: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f67f965b710>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=256, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on test data\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = pred.argmax(axis=1).ravel()\n",
    "actual_y = y_test.argmax(axis=1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          de       0.98      0.98      0.98     89924\n",
      "          en       0.96      0.98      0.97     89480\n",
      "          es       0.99      0.98      0.98     90056\n",
      "          fr       0.98      0.97      0.97     90318\n",
      "          it       0.99      0.98      0.98     90222\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    450000\n",
      "   macro avg       0.98      0.98      0.98    450000\n",
      "weighted avg       0.98      0.98      0.98    450000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(actual_y, pred_y, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(line):\n",
    "    \"\"\"\n",
    "    Prediction method for single line\n",
    "    \"\"\"\n",
    "    line = line.lower()\n",
    "    chars = [c for c in line]\n",
    "    encoded = encode(chars, ch2int)\n",
    "    padded = keras.preprocessing.sequence.pad_sequences([encoded], maxlen=sentense_len, truncating='post', padding='post')\n",
    "    scores = model.predict(padded)\n",
    "    max_index = scores[0].argmax()\n",
    "    lbl = label_encoder.classes_[max_index]\n",
    "    return lbl, scores[0][max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('en', 0.9471939)\n"
     ]
    }
   ],
   "source": [
    "# sample perdiction\n",
    "print(predict('this is my sample text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real time data from google news\n",
    "test_data = [\n",
    "    ('en', 'Today rural India and its villages have declared themselves'),\n",
    "    ('de', 'Es ist einer dieser Momente, bei denen man dabei gewesen sein will'),\n",
    "    ('fr', 'Mais rien ne permet pour l’instant de confirmer ces propos.'),\n",
    "    ('it', 'Il peso della compartecipazione dei cittadini (il ticket appunto) sarà cacolato'),\n",
    "    ('es', 'Después de la evaluación y las pruebas médicas, se descubrió que tenía un')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Data:Today rural India and its villages have declared themselves\n",
      "Predicted:('en', 0.97216403), Actual:en\n",
      "-----------------\n",
      "Data:Es ist einer dieser Momente, bei denen man dabei gewesen sein will\n",
      "Predicted:('de', 0.9998753), Actual:de\n",
      "-----------------\n",
      "Data:Mais rien ne permet pour l’instant de confirmer ces propos.\n",
      "Predicted:('fr', 0.98878455), Actual:fr\n",
      "-----------------\n",
      "Data:Il peso della compartecipazione dei cittadini (il ticket appunto) sarà cacolato\n",
      "Predicted:('it', 0.9981592), Actual:it\n",
      "-----------------\n",
      "Data:Después de la evaluación y las pruebas médicas, se descubrió que tenía un\n",
      "Predicted:('es', 0.9999844), Actual:es\n"
     ]
    }
   ],
   "source": [
    "# predict on real time data\n",
    "for actual_lang, data in test_data:\n",
    "    print('-----------------')\n",
    "    print(f'Data:{data}')\n",
    "    print(f'Predicted:{predict(data)}, Actual:{actual_lang}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
