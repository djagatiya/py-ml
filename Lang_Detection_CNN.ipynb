{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "# core libs\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Embedding, Conv1D, MaxPool1D, Flatten, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras= 2.2.4\n",
      "sklearn= 0.20.4\n",
      "numpy= 1.16.4\n"
     ]
    }
   ],
   "source": [
    "# Library versions\n",
    "print(f'keras= {keras.__version__}')\n",
    "print(f'sklearn= {sklearn.__version__}')\n",
    "print(f'numpy= {np.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Configs\n",
    "data_dir = '/media/divyesh/WorkSpace/Blogs/raw_data'\n",
    "# Select to articles from file\n",
    "num_of_articles = 10000\n",
    "# Maximum sequence length\n",
    "sentense_len = 150\n",
    "# shingle configs\n",
    "shingles_range = (70, 100, 130)\n",
    "# how many shingles generate per line\n",
    "shingle_per_line = 10 \n",
    "# out of vocabulary token\n",
    "oov_str = 'oov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language code wise full name mapping\n",
    "lang_code_dict = {\n",
    "    'en':'english', 'de':'german', \n",
    "    'fr':'french', 'it':'italian', \n",
    "    'es':'spanish'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language code wise data file mapping\n",
    "data_info = {\n",
    "    'en' : data_dir + '/en.txt',\n",
    "    'de' : data_dir + '/de.txt',\n",
    "    'fr' : data_dir + '/fr.txt',\n",
    "    'it' : data_dir + '/it.txt',\n",
    "    'es' : data_dir + '/es.txt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en english /media/divyesh/WorkSpace/Blogs/raw_data/en.txt\n",
      "de german /media/divyesh/WorkSpace/Blogs/raw_data/de.txt\n",
      "fr french /media/divyesh/WorkSpace/Blogs/raw_data/fr.txt\n",
      "it italian /media/divyesh/WorkSpace/Blogs/raw_data/it.txt\n",
      "es spanish /media/divyesh/WorkSpace/Blogs/raw_data/es.txt\n"
     ]
    }
   ],
   "source": [
    "for lang_code, file_path in data_info.items():\n",
    "    print(lang_code, lang_code_dict[lang_code], file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 10000\n",
      "de 10000\n",
      "fr 10000\n",
      "it 10000\n",
      "es 10000\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "data_dict = {}\n",
    "for lang_code, file_path in data_info.items():\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = lines[:num_of_articles]\n",
    "        # convert to lower case\n",
    "        lines = [l.lower().strip() for l in lines]\n",
    "        data_dict[lang_code] = lines\n",
    "        print(lang_code, len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shingles(line, length, total):\n",
    "    \"\"\"\n",
    "    Generate shingles from line\n",
    "    \"\"\"\n",
    "    #todo: USE SET to remove REDUUUUUU\n",
    "    shingle_list = [] \n",
    "    max_index = len(line) - length\n",
    "    if max_index > 0:\n",
    "        for _ in range(total):\n",
    "            index = random.randint(0, max_index)\n",
    "            shingle_text = line[index:index+length]\n",
    "            shingle_list.append(shingle_text)\n",
    "    else:\n",
    "        shingle_list.append(line)\n",
    "    return shingle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shingles_lines(line, length, total):\n",
    "    \"\"\"\n",
    "    Generate shingles from list of lines\n",
    "    \"\"\"\n",
    "    shingle_list = []\n",
    "    for line in lines:\n",
    "        shingles = generate_shingles(line=line, length=length, total=total)\n",
    "        shingle_list.extend(shingles)\n",
    "    return shingle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 300000\n",
      "de 300000\n",
      "fr 300000\n",
      "it 300000\n",
      "es 300000\n"
     ]
    }
   ],
   "source": [
    "# generate shingles\n",
    "shingle_data_dict = {}\n",
    "for lang, lines in data_dict.items():\n",
    "    shingle_list = []\n",
    "    for s_range in shingles_range:\n",
    "        shingles = generate_shingles_lines(lines, s_range, shingle_per_line)\n",
    "        shingle_list.extend(shingles)\n",
    "    shingle_data_dict[lang] = shingle_list\n",
    "    print(lang, len(shingle_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500000 1500000\n"
     ]
    }
   ],
   "source": [
    "# create list of lines and labels\n",
    "data_lines, labels = [], []\n",
    "for lang, samples in shingle_data_dict.items():\n",
    "    data_lines.extend(samples)\n",
    "    total_samples = len(samples)\n",
    "    labels.extend([lang] * total_samples)\n",
    "print(len(data_lines), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'e', 'a', 'i', 'n', 'r', 't', 's', 'o', 'l', 'd', 'c', 'u', 'm', 'p', 'g', 'h', 'b', 'f', 'v', 'w', 'z', 'y', 'k', 'é', 'q', 'j', 'x', 'ó', 'í', 'ü', 'á', 'ä', 'è', 'ö', 'à', 'ñ', 'ú', 'ß', 'ò', 'ç', 'ù', 'ê', 'ô', 'â', 'î', 'ì', 'œ', 'û', 'ï', 'ō', '²', 'š', 'ë', 'č', 'ã', 'ł', 'ā', 'ø', 'ć', 'ū', 'ž', 'ı', 'å', 'ř', 'ş', 'ý', 'æ', 'α', 'ο', 'ă', 'о', 'а', 'ń', 'н', 'ν', 'oov']\n"
     ]
    }
   ],
   "source": [
    "# create list of all characters from all data lines\n",
    "data_char_ls = []\n",
    "for line in data_lines:\n",
    "    char_ls = [c for c in line]\n",
    "    data_char_ls.append(char_ls)\n",
    "    \n",
    "# count all characters\n",
    "cunt = Counter(x for xs in data_char_ls for x in set(xs))\n",
    "\n",
    "# create vocabulary\n",
    "char_vocab = [c[0] for c in cunt.most_common(76)] + [oov_str]\n",
    "print(char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'e': 2, 'a': 3, 'i': 4, 'n': 5, 'r': 6, 't': 7, 's': 8, 'o': 9, 'l': 10, 'd': 11, 'c': 12, 'u': 13, 'm': 14, 'p': 15, 'g': 16, 'h': 17, 'b': 18, 'f': 19, 'v': 20, 'w': 21, 'z': 22, 'y': 23, 'k': 24, 'é': 25, 'q': 26, 'j': 27, 'x': 28, 'ó': 29, 'í': 30, 'ü': 31, 'á': 32, 'ä': 33, 'è': 34, 'ö': 35, 'à': 36, 'ñ': 37, 'ú': 38, 'ß': 39, 'ò': 40, 'ç': 41, 'ù': 42, 'ê': 43, 'ô': 44, 'â': 45, 'î': 46, 'ì': 47, 'œ': 48, 'û': 49, 'ï': 50, 'ō': 51, '²': 52, 'š': 53, 'ë': 54, 'č': 55, 'ã': 56, 'ł': 57, 'ā': 58, 'ø': 59, 'ć': 60, 'ū': 61, 'ž': 62, 'ı': 63, 'å': 64, 'ř': 65, 'ş': 66, 'ý': 67, 'æ': 68, 'α': 69, 'ο': 70, 'ă': 71, 'о': 72, 'а': 73, 'ń': 74, 'н': 75, 'ν': 76, 'oov': 77}\n",
      "\n",
      "{1: ' ', 2: 'e', 3: 'a', 4: 'i', 5: 'n', 6: 'r', 7: 't', 8: 's', 9: 'o', 10: 'l', 11: 'd', 12: 'c', 13: 'u', 14: 'm', 15: 'p', 16: 'g', 17: 'h', 18: 'b', 19: 'f', 20: 'v', 21: 'w', 22: 'z', 23: 'y', 24: 'k', 25: 'é', 26: 'q', 27: 'j', 28: 'x', 29: 'ó', 30: 'í', 31: 'ü', 32: 'á', 33: 'ä', 34: 'è', 35: 'ö', 36: 'à', 37: 'ñ', 38: 'ú', 39: 'ß', 40: 'ò', 41: 'ç', 42: 'ù', 43: 'ê', 44: 'ô', 45: 'â', 46: 'î', 47: 'ì', 48: 'œ', 49: 'û', 50: 'ï', 51: 'ō', 52: '²', 53: 'š', 54: 'ë', 55: 'č', 56: 'ã', 57: 'ł', 58: 'ā', 59: 'ø', 60: 'ć', 61: 'ū', 62: 'ž', 63: 'ı', 64: 'å', 65: 'ř', 66: 'ş', 67: 'ý', 68: 'æ', 69: 'α', 70: 'ο', 71: 'ă', 72: 'о', 73: 'а', 74: 'ń', 75: 'н', 76: 'ν', 77: 'oov'}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary for (char to index)\n",
    "# here we (index + 1) becoz, 0 index for padding\n",
    "ch2int = {c:i+1 for i, c in enumerate(char_vocab)}\n",
    "print(ch2int)\n",
    "print()\n",
    "# create dictionary for (index to char)\n",
    "int2ch = {i:c for c, i in ch2int.items()}\n",
    "print(int2ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(in_ls, key):\n",
    "    \"\"\"\n",
    "    encode list of character to index of characters using 'char2int' dictionary\n",
    "    \"\"\"\n",
    "    out_ls = []\n",
    "    for ch in in_ls:\n",
    "        index = key.get(ch)\n",
    "        if index is None:\n",
    "            index = key.get(oov_str)\n",
    "        out_ls.append(index)\n",
    "    return out_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500000\n"
     ]
    }
   ],
   "source": [
    "# data encoding\n",
    "encoded_ls = [encode(l, ch2int) for l in data_lines]\n",
    "print(len(encoded_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15790"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe count 'oov' in dataset\n",
    "counts = 0\n",
    "for enc in encoded_ls:\n",
    "    if ch2int[oov_str] in enc:\n",
    "        counts += 1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding and trucating of encoded sequence\n",
    "X = pad_sequences(encoded_ls, maxlen=sentense_len, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de' 'en' 'es' 'fr' 'it']\n"
     ]
    }
   ],
   "source": [
    "# target encoding from 'en' or 'de' language code to 0, 1 \n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of targets\n",
    "y = to_categorical(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500000, 150) (1500000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Test split (70:30) ratio from full data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050000, 150) (450000, 150) (1050000, 5) (450000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/divyesh/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 32)           2496      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 148, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 146, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 46, 128)           24704     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 44, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 14, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               459008    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 555,333\n",
      "Trainable params: 555,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyesh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Build the Neural network\n",
    "inp = Input(shape=(sentense_len, ))\n",
    "x = Embedding(input_dim=len(char_vocab) + 1, output_dim=32)(inp)\n",
    "x = Conv1D(64, 3, activation='relu')(x)\n",
    "x = Conv1D(64, 3, activation='relu')(x)\n",
    "x = MaxPool1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPool1D(3)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(5, activation='softmax')(x)\n",
    "model = Model(inputs=inp, output=x)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/divyesh/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1050000 samples, validate on 450000 samples\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 1346s 1ms/step - loss: 0.1331 - acc: 0.9613 - val_loss: 0.0945 - val_acc: 0.9739\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 1362s 1ms/step - loss: 0.0810 - acc: 0.9777 - val_loss: 0.0769 - val_acc: 0.9790\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 1345s 1ms/step - loss: 0.0714 - acc: 0.9802 - val_loss: 0.0745 - val_acc: 0.9799\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 1240s 1ms/step - loss: 0.0656 - acc: 0.9818 - val_loss: 0.0736 - val_acc: 0.9801\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 1223s 1ms/step - loss: 0.0609 - acc: 0.9829 - val_loss: 0.0770 - val_acc: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e4fb0f710>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on test data\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = pred.argmax(axis=1).ravel()\n",
    "actual_y = y_test.argmax(axis=1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          de       0.97      0.98      0.98     89924\n",
      "          en       0.98      0.97      0.97     89480\n",
      "          es       0.99      0.98      0.99     90056\n",
      "          fr       0.97      0.98      0.97     90318\n",
      "          it       0.98      0.98      0.98     90222\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    450000\n",
      "   macro avg       0.98      0.98      0.98    450000\n",
      "weighted avg       0.98      0.98      0.98    450000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(actual_y, pred_y, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(line):\n",
    "    \"\"\"\n",
    "    Prediction method for single line\n",
    "    \"\"\"\n",
    "    line = line.lower()\n",
    "    chars = [c for c in line]\n",
    "    encoded = encode(chars, ch2int)\n",
    "    padded = keras.preprocessing.sequence.pad_sequences([encoded], maxlen=sentense_len, truncating='post', padding='post')\n",
    "    scores = model.predict(padded)\n",
    "    max_index = scores[0].argmax()\n",
    "    lbl = label_encoder.classes_[max_index]\n",
    "    return lbl, scores[0][max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('en', 0.80307883)\n"
     ]
    }
   ],
   "source": [
    "# sample perdiction\n",
    "print(predict('this is sample text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real time data from google news\n",
    "test_data = [\n",
    "    ('en', 'Today rural India and its villages have declared themselves'),\n",
    "    ('de', 'Es ist einer dieser Momente, bei denen man dabei gewesen sein will'),\n",
    "    ('fr', 'Mais rien ne permet pour l’instant de confirmer ces propos.'),\n",
    "    ('it', 'Il peso della compartecipazione dei cittadini (il ticket appunto) sarà cacolato'),\n",
    "    ('es', 'Después de la evaluación y las pruebas médicas, se descubrió que tenía un')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Data:Today rural India and its villages have declared themselves\n",
      "Predicted:('en', 0.9981042), Actual:en\n",
      "-----------------\n",
      "Data:Es ist einer dieser Momente, bei denen man dabei gewesen sein will\n",
      "Predicted:('de', 0.99996316), Actual:de\n",
      "-----------------\n",
      "Data:Mais rien ne permet pour l’instant de confirmer ces propos.\n",
      "Predicted:('fr', 0.9699458), Actual:fr\n",
      "-----------------\n",
      "Data:Il peso della compartecipazione dei cittadini (il ticket appunto) sarà cacolato\n",
      "Predicted:('it', 0.99970454), Actual:it\n",
      "-----------------\n",
      "Data:Después de la evaluación y las pruebas médicas, se descubrió que tenía un\n",
      "Predicted:('es', 0.999961), Actual:es\n"
     ]
    }
   ],
   "source": [
    "# predict on real time data\n",
    "for actual_lang, data in test_data:\n",
    "    print('-----------------')\n",
    "    print(f'Data:{data}')\n",
    "    print(f'Predicted:{predict(data)}, Actual:{actual_lang}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
